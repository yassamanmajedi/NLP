{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "981ce393",
   "metadata": {
    "id": "981ce393"
   },
   "source": [
    "# Introduction to Natural Language Processing: Assignment 2\n",
    "\n",
    "In this exercise we'll practice features extraction using Tf-Idf and SpaCy as well as multiclass text classification using the word embedding technique.\n",
    "\n",
    "- You can use built-in Python packages, spaCy, scikit-learn, Numpy and Pandas.\n",
    "- Please comment your code\n",
    "- Submissions are due Tuesdays at 23:59 **only** on eCampus: **Assignmnets >> Student Submissions >> Assignment 3 (Deadline: 12.11.2024, at 23:59)**\n",
    "\n",
    "- Name the file aproppriately: \"Assignment_2_\\<Your_Name\\>.ipynb\" and submit only the Jupyter Notebook file.\n",
    "- If you are working in a group of two, please have the names of both of the members in the file name.\n",
    "- Please use relative path, your code should work on my computer if the Jupyter Notebook and the file are both in the same directory.\n",
    "\n",
    "Example: file_name = bbc-news.csv, **DON'T use:** /Users/ComputerName/Username/Documents/.../bbc-news.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1325c5b",
   "metadata": {
    "id": "d1325c5b"
   },
   "source": [
    "### Task 1 (2 points)\n",
    "\n",
    "Write a function `extract_proper_nouns(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a list containing all proper nouns with more than one token.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "text = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\n",
    "\n",
    "return = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d26341",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEPERM: operation not permitted, scandir '/Users/yasamanmajedi/Downloads/Assignment 2 (Deadline_ 12.11.2024, at 23_59)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92133a54",
   "metadata": {
    "id": "92133a54"
   },
   "outputs": [],
   "source": [
    "def extract_proper_nouns(my_file_name):\n",
    "    several_token_propn = []\n",
    "    # here comes your code\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    with open(my_file_name, 'r') as file:\n",
    "        text = file.read()\n",
    "    doc = nlp(text)\n",
    "    for entity in doc.ents:\n",
    "        if len(entity.text.split()) > 1:\n",
    "            several_token_propn.append(entity.text)\n",
    "\n",
    "    return(several_token_propn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a669e",
   "metadata": {
    "id": "8d0a669e"
   },
   "source": [
    "### Task 2 (3 points)\n",
    "\n",
    "Write a function `common_lemma(my_file_name)` that takes a file name (my_file_name.txt) as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "1.\n",
    "text = \"When users google for a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\n",
    "\n",
    "return = `{\"query\": [\"query\", \"querying\"]}`\n",
    "\n",
    "2.\n",
    "text = I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\n",
    "\n",
    "return = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab5733",
   "metadata": {
    "id": "8fab5733"
   },
   "outputs": [],
   "source": [
    "def common_lemma(my_file_name):\n",
    "    tokens_with_common_lemma = {}\n",
    "    # here comes your code\n",
    "    return(tokens_with_common_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d27e0-56fc-40b9-b510-b2d94c90bcf3",
   "metadata": {
    "id": "d46d27e0-56fc-40b9-b510-b2d94c90bcf3"
   },
   "source": [
    "### Task 3 (1 point)\n",
    "\n",
    "Load the data `bbc-text.csv`; This datata consists of 2225 documents from the BBC news website corresponding to stories from 2004-2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac56bf9-aba8-48c7-9dcd-a46d53426f82",
   "metadata": {
    "id": "fac56bf9-aba8-48c7-9dcd-a46d53426f82"
   },
   "outputs": [],
   "source": [
    "# Here comes your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6bf1c-a802-4ca6-a74e-ace4fd914626",
   "metadata": {
    "id": "32d6bf1c-a802-4ca6-a74e-ace4fd914626"
   },
   "source": [
    "### Task 4 (1 point)\n",
    "\n",
    "Show how many articles we have for each topical area (class label) in the dataset using a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ef9d8-52c2-4508-ad94-91525476d9a4",
   "metadata": {
    "id": "c93ef9d8-52c2-4508-ad94-91525476d9a4"
   },
   "outputs": [],
   "source": [
    "# Here comes your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd667360-ae62-4f70-8236-9e9d11ab29d7",
   "metadata": {
    "id": "cd667360-ae62-4f70-8236-9e9d11ab29d7"
   },
   "source": [
    "### Task 5 (2 point)\n",
    "\n",
    "Preprocessing: Define two following functions and apply them to the dataset:\n",
    "1. Remove punctuation\n",
    "2. Remove any numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d69ff-c80a-468e-a6be-af5538a8c2b9",
   "metadata": {
    "id": "8f3d69ff-c80a-468e-a6be-af5538a8c2b9"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(corpus):\n",
    "    # Here comes your code\n",
    "    return(cleaned_corpus)\n",
    "\n",
    "def remove_numbers(corpus):\n",
    "    # Here comes your code\n",
    "    return(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0df6be",
   "metadata": {},
   "source": [
    "### Task 6.1 (1 points)\n",
    "\n",
    "Split the data into training and test set (70% and 30%) using scikit-learn, shuffle it, and set the `seed=101 (random_state)`.\n",
    "\n",
    "**NOTE:** If working with this dataset is not computationaly possible for you, you can work only with a subset of the dataset (i.e., the first 1000 rows) and use only the first 150 tokens for each article. You should point this out in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3619b0ed-2213-41de-bf9f-95c97344f038",
   "metadata": {
    "id": "3619b0ed-2213-41de-bf9f-95c97344f038"
   },
   "outputs": [],
   "source": [
    "# Here comes your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9e7e25",
   "metadata": {},
   "source": [
    "### Task 6.2 (3 points)\n",
    "\n",
    "### **Training models on TF-IDF vectors:**\n",
    "\n",
    "a) Convert each article in your data splits to a vector representation using the tf-idf-vectorizer.\n",
    "\n",
    "b) Using the vectors from the previous step, train the `MLPClassifier` and another model of your choice from the scikit-learn library.\n",
    "\n",
    "c) Test both of your models on the test set from Task 6.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667054a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here comes your code\n",
    "X_train_tfidf = ...\n",
    "\n",
    "y_pred_MLP_tfidf = ...\n",
    "y_pred_<MODEL>_tfidf = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cef728",
   "metadata": {},
   "source": [
    "### Task 6.3 (3 points)\n",
    "\n",
    "### **Training models on SpaCy model vector representation:**\n",
    "\n",
    "a) Convert each article in your data splits to a vector representation using the pre-trained spaCy model. (**Hint:** It should be stored as an array)\n",
    "\n",
    "b) Using the vectors from the previous step, train the `MLPClassifier` and another model of your choice from the scikit-learn library.\n",
    "\n",
    "c) Test both of your models on the test set from Task 6.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here comes your code\n",
    "X_train_spacy = ...\n",
    "\n",
    "y_pred_MLP_spacy = ...\n",
    "y_pred_<MODEL>_spacy = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a53269-9dc6-43a8-a071-dc596beee0cf",
   "metadata": {
    "id": "88a53269-9dc6-43a8-a071-dc596beee0cf"
   },
   "source": [
    "### Task 7 (4 points)\n",
    "\n",
    "Using the predictions from the four classifiers, evaluate the models and report accuracy, recall, precision, f1 scores and confusion matrix for each of them. (**Hint:** You should build a confusion matrix for multi-class classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80276a8-e395-4015-a5d8-00fa7b8e051b",
   "metadata": {
    "id": "d80276a8-e395-4015-a5d8-00fa7b8e051b"
   },
   "outputs": [],
   "source": [
    "# Here comes your code"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
